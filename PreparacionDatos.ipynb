{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in /home/valentina/anaconda3/lib/python3.7/site-packages (0.1.72)\r\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /home/valentina/anaconda3/lib/python3.7/site-packages (from contractions) (0.0.24)\r\n",
      "Requirement already satisfied: anyascii in /home/valentina/anaconda3/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\r\n",
      "Requirement already satisfied: pyahocorasick in /home/valentina/anaconda3/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\r\n"
     ]
    }
   ],
   "source": [
    "# Librería para manejar las contracciones que se presentan en el inglés.\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/0f/c51780fb99b156e998a8bcbc418aea9179ccd301f8c2a8c1bb255c294af6/inflect-6.0.0-py3-none-any.whl\n",
      "Collecting pydantic (from inflect)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/ec/230ab377c457cd68cfda78759e2a57f8c08a9e9adb4cd53c4d2fc9100b15/pydantic-1.10.2-py3-none-any.whl (154kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 788kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4.1.0 (from pydantic->inflect)\n",
      "  Downloading https://files.pythonhosted.org/packages/0b/8e/f1a0a5a76cfef77e1eb6004cb49e5f8d72634da638420b9ea492ce8305e8/typing_extensions-4.4.0-py3-none-any.whl\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement typing-extensions~=3.7.4, but you'll have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: typing-extensions, pydantic, inflect\n",
      "  Found existing installation: typing-extensions 3.7.4.3\n",
      "    Uninstalling typing-extensions-3.7.4.3:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.3\n",
      "Successfully installed inflect-6.0.0 pydantic-1.10.2 typing-extensions-4.4.0\n",
      "Collecting pandas-profiling==2.7.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/8a/25da481171f4912e2515a76fe31b7a4f036a443b8858b244ef7daaffd5b6/pandas_profiling-2.7.1-py2.py3-none-any.whl (252kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 47kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scipy>=1.4.1 (from pandas-profiling==2.7.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/58/4f/11f34cfc57ead25752a7992b069c36f5d18421958ebd6466ecd849aeaf86/scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
      "Collecting astropy>=4.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/6e/04fba8c047000e3d9f09879f4e24ff805edbc4bb3943ec3a31e18ed6cad4/astropy-4.3.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (10.7MB)\n",
      "\u001b[K     |████████████████████████████████| 10.7MB 764kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.43.0 in /home/valentina/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (4.63.0)\n",
      "Collecting pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/f0/f99700ef327e51d291efdf4a6de29e685c4d198cbf8531541fc84d169e0e/pandas-1.3.5.tar.gz (4.7MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7MB 847kB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting visions[type_image_path]==0.4.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/fe/7614dec3db3f20882ff12dae0a58b579e97b590f2994ce9c953fe179d512/visions-0.4.1-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib>=3.2.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/62/7b662284352867a86acfb636761ba351723fc3a235efd8397578d903413d/matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2MB)\n",
      "\u001b[K     |████████████████████████████████| 11.2MB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /home/valentina/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (1.19.5)\n",
      "Collecting tangled-up-in-unicode>=0.0.4 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/60/3651960b74aead282ec1ad819e70bdccf3ee73322d13d4339a6e3f5b7ed3/tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2>=2.11.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting phik>=0.9.10 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/6c/2ec53b5ea4cb6aee603de707599f30aada1ff3264b954fd8424e2ad40964/phik-0.12.2.tar.gz (602kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 2.4MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting htmlmin>=0.1.12 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/e7/fcd59e12169de19f0131ff2812077f964c6b960e7c09804d30a7bf2ab461/htmlmin-0.1.12.tar.gz\n",
      "Collecting ipywidgets>=7.5.1 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/56/990c10ca8751182ace2464cb0e4baafb7087a40c185c9142b9cd18683fac/ipywidgets-8.0.2-py3-none-any.whl (134kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 475kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting missingno>=0.4.2 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/a2/be45b3bd2fe14cf9173f2337ab87a0f877d6847cf097e641eab4811a8b02/missingno-0.5.1-py3-none-any.whl\n",
      "Collecting requests>=2.23.0 (from pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/91/6d9b8ccacd0412c08820f72cebaa4f0c0441b5cda699c90f618b6f8a1b42/requests-2.28.1-py3-none-any.whl (62kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.8MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/valentina/anaconda3/lib/python3.7/site-packages (from pandas-profiling==2.7.1) (0.13.2)\n",
      "Collecting confuse>=1.0.0 (from pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/77/db/3c0594c0341d1dd095d685c73f49507b2e5392c7588fd893d07a4c5d959f/confuse-2.0.0-py3-none-any.whl\n",
      "Collecting pyerfa>=1.7.3 (from astropy>=4.0->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/76/68d862db9bd200776a3fa60d2c07fcee34285e5363adb88fdd8fbc3bce36/pyerfa-2.0.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (746kB)\n",
      "\u001b[K     |████████████████████████████████| 747kB 942kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version == \"3.7\" in /home/valentina/anaconda3/lib/python3.7/site-packages (from astropy>=4.0->pandas-profiling==2.7.1) (0.17)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/valentina/anaconda3/lib/python3.7/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/valentina/anaconda3/lib/python3.7/site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (2.8.0)\n",
      "Collecting attrs>=19.3.0 (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/bc/d817287d1aa01878af07c19505fafd1165cd6a119e9d0821ca1d1c20312d/attrs-22.1.0-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.4 (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/93/aa6613aa70d6eb4868e667068b5a11feca9645498fd31b954b6c4bb82fa5/networkx-2.6.3-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 587kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imagehash; extra == \"type_image_path\" (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/b4/19a746a986c6e38595fa5947c028b1b8e287773dcad766e648897ad2a4cf/ImageHash-4.3.1-py2.py3-none-any.whl (296kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow; extra == \"type_image_path\" in /home/valentina/anaconda3/lib/python3.7/site-packages (from visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (6.1.0)\n",
      "Collecting packaging>=20.0 (from matplotlib>=3.2.0->pandas-profiling==2.7.1)\n",
      "  Using cached https://files.pythonhosted.org/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/valentina/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (2.4.0)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.2.0->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/73/d8f2d961ecd548685d770fb005e355514573d2108d8ed9460d7a1f1870b5/fonttools-4.37.4-py3-none-any.whl (960kB)\n",
      "\u001b[K     |████████████████████████████████| 962kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /home/valentina/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/valentina/anaconda3/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling==2.7.1) (0.10.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.1->pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/97/2288fe498044284f39ab8950703e88abbac2abbdf65524d576157af70556/MarkupSafe-2.1.1.tar.gz\n",
      "Collecting widgetsnbextension~=4.0 (from ipywidgets>=7.5.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/ae/ee70b20dc836d935a9a6483339854c09d8752e55a8104668e2426cf3baf3/widgetsnbextension-4.0.3-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 3.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jupyterlab-widgets~=3.0 (from ipywidgets>=7.5.1->pandas-profiling==2.7.1)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/52/2f4b8f5975312fb58f4eacab2e6f6cfd2efd05704514a60a151a4e69d608/jupyterlab_widgets-3.0.3-py3-none-any.whl (384kB)\n",
      "\u001b[K     |████████████████████████████████| 389kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipykernel>=4.5.1 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.3.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling==2.7.1) (7.6.1)\n",
      "Requirement already satisfied: seaborn in /home/valentina/anaconda3/lib/python3.7/site-packages (from missingno>=0.4.2->pandas-profiling==2.7.1) (0.9.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/valentina/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (1.24.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/valentina/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/valentina/anaconda3/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling==2.7.1) (2019.6.16)\n",
      "Collecting charset-normalizer<3,>=2 (from requests>=2.23.0->pandas-profiling==2.7.1)\n",
      "  Downloading https://files.pythonhosted.org/packages/db/51/a507c856293ab05cdc1db77ff4bc1268ddd39f29e7dc4919aa497f0adbec/charset_normalizer-2.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /home/valentina/anaconda3/lib/python3.7/site-packages (from confuse>=1.0.0->pandas-profiling==2.7.1) (5.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/valentina/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version == \"3.7\"->astropy>=4.0->pandas-profiling==2.7.1) (0.5.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/valentina/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas!=1.0.0,!=1.0.1,!=1.0.2,>=0.25.3->pandas-profiling==2.7.1) (1.15.0)\n",
      "Requirement already satisfied: PyWavelets in /home/valentina/anaconda3/lib/python3.7/site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]==0.4.1->pandas-profiling==2.7.1) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /home/valentina/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=3.2.0->pandas-profiling==2.7.1) (41.0.1)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (6.0.3)\n",
      "Requirement already satisfied: jupyter-client in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (5.3.1)\n",
      "Requirement already satisfied: decorator in /home/valentina/anaconda3/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /home/valentina/anaconda3/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.2.0)\n",
      "Requirement already satisfied: pygments in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.4.2)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.7.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (2.0.9)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.13.3)\n",
      "Requirement already satisfied: pickleshare in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.7.5)\n",
      "Requirement already satisfied: backcall in /home/valentina/anaconda3/lib/python3.7/site-packages (from ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.1.0)\n",
      "Requirement already satisfied: jupyter-core in /home/valentina/anaconda3/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (4.5.0)\n",
      "Requirement already satisfied: pyzmq>=13 in /home/valentina/anaconda3/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (18.0.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/valentina/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /home/valentina/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in /home/valentina/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=6.1.0->ipywidgets>=7.5.1->pandas-profiling==2.7.1) (0.5.0)\n",
      "Building wheels for collected packages: pandas, phik\n",
      "  Building wheel for pandas (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/valentina/.cache/pip/wheels/5c/f4/45/389dc711f0c5ff9adeb5245397ab18bf75182e8cff9fbfa916\n",
      "  Building wheel for phik (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/valentina/.cache/pip/wheels/61/73/87/197f78ba68ebfa7f4b39a883d817fe2ed5b14c6ef7a06452b8\n",
      "Successfully built pandas phik\n",
      "Building wheels for collected packages: htmlmin, MarkupSafe\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/valentina/.cache/pip/wheels/43/07/ac/7c5a9d708d65247ac1f94066cf1db075540b85716c30255459\n",
      "  Building wheel for MarkupSafe (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/valentina/.cache/pip/wheels/f5/40/34/d60ef965622011684037ea53e53fd44ef58ed2062f26878ce2\n",
      "Successfully built htmlmin MarkupSafe\n",
      "\u001b[31mERROR: conda 4.10.1 requires ruamel_yaml_conda>=0.11.14, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: matplotlib 3.5.3 has requirement pillow>=6.2.0, but you'll have pillow 6.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: phik 0.12.2 has requirement joblib>=0.14.1, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scipy, pyerfa, astropy, pandas, tangled-up-in-unicode, attrs, networkx, imagehash, visions, packaging, fonttools, matplotlib, MarkupSafe, jinja2, phik, htmlmin, widgetsnbextension, jupyterlab-widgets, ipywidgets, missingno, charset-normalizer, requests, confuse, pandas-profiling\n",
      "  Found existing installation: scipy 1.3.0\n",
      "    Uninstalling scipy-1.3.0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Successfully uninstalled scipy-1.3.0\n",
      "  Found existing installation: astropy 3.2.1\n",
      "    Uninstalling astropy-3.2.1:\n",
      "      Successfully uninstalled astropy-3.2.1\n",
      "  Found existing installation: pandas 0.24.2\n",
      "    Uninstalling pandas-0.24.2:\n",
      "      Successfully uninstalled pandas-0.24.2\n",
      "  Found existing installation: attrs 19.1.0\n",
      "    Uninstalling attrs-19.1.0:\n",
      "      Successfully uninstalled attrs-19.1.0\n",
      "  Found existing installation: networkx 2.3\n",
      "    Uninstalling networkx-2.3:\n",
      "      Successfully uninstalled networkx-2.3\n",
      "  Found existing installation: packaging 19.0\n",
      "    Uninstalling packaging-19.0:\n",
      "      Successfully uninstalled packaging-19.0\n",
      "  Found existing installation: matplotlib 3.1.0\n",
      "    Uninstalling matplotlib-3.1.0:\n",
      "      Successfully uninstalled matplotlib-3.1.0\n",
      "  Found existing installation: MarkupSafe 1.1.1\n",
      "    Uninstalling MarkupSafe-1.1.1:\n",
      "      Successfully uninstalled MarkupSafe-1.1.1\n",
      "  Found existing installation: Jinja2 2.10.1\n",
      "    Uninstalling Jinja2-2.10.1:\n",
      "      Successfully uninstalled Jinja2-2.10.1\n",
      "  Found existing installation: widgetsnbextension 3.5.0\n",
      "    Uninstalling widgetsnbextension-3.5.0:\n",
      "      Successfully uninstalled widgetsnbextension-3.5.0\n",
      "  Found existing installation: ipywidgets 7.5.0\n",
      "    Uninstalling ipywidgets-7.5.0:\n",
      "      Successfully uninstalled ipywidgets-7.5.0\n",
      "  Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "Successfully installed MarkupSafe-2.1.1 astropy-4.3.1 attrs-22.1.0 charset-normalizer-2.1.1 confuse-2.0.0 fonttools-4.37.4 htmlmin-0.1.12 imagehash-4.3.1 ipywidgets-8.0.2 jinja2-3.1.2 jupyterlab-widgets-3.0.3 matplotlib-3.5.3 missingno-0.5.1 networkx-2.6.3 packaging-21.3 pandas-1.3.5 pandas-profiling-2.7.1 phik-0.12.2 pyerfa-2.0.0.1 requests-2.28.1 scipy-1.7.3 tangled-up-in-unicode-0.2.0 visions-0.4.1 widgetsnbextension-4.0.3\n"
     ]
    }
   ],
   "source": [
    " # librería para manejar las flexiones gramaticales en el idioma inglés.\n",
    "!pip install inflect\n",
    "!pip install pandas-profiling==2.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/valentina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # librería Natural Language Toolkit, usada para trabajar con textos \n",
    "import nltk\n",
    "# Punkt permite separar un texto en frases.\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/valentina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descarga todas las palabras vacias, es decir, aquellas que no aportan nada al significado del texto\n",
    "# ¿Cuales son esas palabras vacías?\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/valentina/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Descarga de paquete WordNetLemmatizer, este es usado para encontrar el lema de cada palabra\n",
    "# ¿Qué es el lema de una palabra? ¿Qué tan dificil puede ser obtenerlo, piensa en el caso en que tuvieras que escribir la función que realiza esta tarea?\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting escape\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/9c/72a72a1a5f16772bcfee1293304d073f776859feded195d020c6f3de37c1/escape-1.1-py2.py3-none-any.whl\n",
      "Installing collected packages: escape\n",
      "Successfully installed escape-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valentina/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "#from pandas_profiling import ProfileReport\n",
    "\n",
    "import re, string, unicodedata\n",
    "import contractions\n",
    "import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, plot_precision_recall_curve\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfilamiento y entendimiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Uso de la libreria pandas para la lectura de archivos\n",
    "data = pd.read_csv('DatosSuicidio/SuicidiosProyecto.csv', sep=',', encoding = 'utf-8')\n",
    "# Asignación a una nueva variable de los datos leidos\n",
    "data_t=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173271</td>\n",
       "      <td>i want to destroy myselffor once everything wa...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>336321</td>\n",
       "      <td>I kinda got behind schedule with learning for ...</td>\n",
       "      <td>non-suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256637</td>\n",
       "      <td>I'm just not sure anymoreFirst and foremost: I...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303772</td>\n",
       "      <td>please give me a reason to liveThats too much ...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>293747</td>\n",
       "      <td>27f struggling to find meaning moving forwardI...</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text        class\n",
       "0      173271  i want to destroy myselffor once everything wa...      suicide\n",
       "1      336321  I kinda got behind schedule with learning for ...  non-suicide\n",
       "2      256637  I'm just not sure anymoreFirst and foremost: I...      suicide\n",
       "3      303772  please give me a reason to liveThats too much ...      suicide\n",
       "4      293747  27f struggling to find meaning moving forwardI...      suicide"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendimiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mode\n",
    "\n",
    "textos = data_t.copy()\n",
    "textos['Conteo'] = [len(x) for x in textos['text']]\n",
    "textos['Moda'] = [[mode([len(x) for x in i.split(' ')])][0] for i in textos['text']]\n",
    "textos['Max'] = [[max([len(x) for x in i.split(' ')])][0] for i in textos['text']]\n",
    "textos['Min'] = [[min([len(x) for x in i.split(' ')])][0] for i in textos['text']]\n",
    "\n",
    "# Se realiza un perfilamiento de los datos con la librería pandas profiling\n",
    "ProfileReport(textos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = words.lower()        \n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in words if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def preprocessing(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 147648/195700 [00:08<00:02, 17828.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İ am losing my mind...İ dont know how i can endure this bullshit ...  \n",
      "İam 21 and suffered almost every stage of my life , things are not going on my way , worst thing is everyone hates me even my family too . They think iam a failure.\n",
      "İam an university student but my grades like an  rotten apple on the tree... i have no motivation or energy. And dont have a girlfriend still virgin . Why i should keep up for nothing ?,  for more suffer ? or more failure ?\n",
      "İ just want peace , love and some money...\n",
      "İ know there is still some hope but i tired keep fighting it is pointless , i hate it i just want some victory . İ am looking for a gun but it is hard to access on my country . İ just dont want hurt anymore... it is enough.  İf people interested in motivational videos please watch \n",
      "(Why we choose suicide Mark Henic) it relaxed me one bit . İ need your prays too\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195700/195700 [00:10<00:00, 18045.49it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(range(len(data_t))):\n",
    "    try:\n",
    "        contractions.fix(data_t['text'][t])\n",
    "    except:\n",
    "        print(data_t['text'][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195700 [00:00<?, ?it/s]/home/valentina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      " 74%|███████▍  | 144584/195700 [10:53<03:37, 235.47it/s]/home/valentina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/home/valentina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/home/valentina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      " 74%|███████▍  | 144658/195700 [10:53<03:34, 237.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'a', 'm', ' ', 'l', 'o', 's', 'i', 'n', 'g', ' ', 'm', 'y', ' ', 'm', 'i', 'n', 'd', '.', '.', '.', 'I', ' ', 'd', 'o', 'n', 't', ' ', 'k', 'n', 'o', 'w', ' ', 'h', 'o', 'w', ' ', 'i', ' ', 'c', 'a', 'n', ' ', 'e', 'n', 'd', 'u', 'r', 'e', ' ', 't', 'h', 'i', 's', ' ', 'b', 'u', 'l', 'l', 's', 'h', 'i', 't', ' ', '.', '.', '.', ' ', ' ', '\\n', 'I', 'a', 'm', ' ', '2', '1', ' ', 'a', 'n', 'd', ' ', 's', 'u', 'f', 'f', 'e', 'r', 'e', 'd', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'e', 'v', 'e', 'r', 'y', ' ', 's', 't', 'a', 'g', 'e', ' ', 'o', 'f', ' ', 'm', 'y', ' ', 'l', 'i', 'f', 'e', ' ', ',', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'a', 'r', 'e', ' ', 'n', 'o', 't', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 'o', 'n', ' ', 'm', 'y', ' ', 'w', 'a', 'y', ' ', ',', ' ', 'w', 'o', 'r', 's', 't', ' ', 't', 'h', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'e', 'v', 'e', 'r', 'y', 'o', 'n', 'e', ' ', 'h', 'a', 't', 'e', 's', ' ', 'm', 'e', ' ', 'e', 'v', 'e', 'n', ' ', 'm', 'y', ' ', 'f', 'a', 'm', 'i', 'l', 'y', ' ', 't', 'o', 'o', ' ', '.', ' ', 'T', 'h', 'e', 'y', ' ', 't', 'h', 'i', 'n', 'k', ' ', 'i', 'a', 'm', ' ', 'a', ' ', 'f', 'a', 'i', 'l', 'u', 'r', 'e', '.', '\\n', 'I', 'a', 'm', ' ', 'a', 'n', ' ', 'u', 'n', 'i', 'v', 'e', 'r', 's', 'i', 't', 'y', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', ' ', 'b', 'u', 't', ' ', 'm', 'y', ' ', 'g', 'r', 'a', 'd', 'e', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'a', 'n', ' ', ' ', 'r', 'o', 't', 't', 'e', 'n', ' ', 'a', 'p', 'p', 'l', 'e', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 't', 'r', 'e', 'e', '.', '.', '.', ' ', 'i', ' ', 'h', 'a', 'v', 'e', ' ', 'n', 'o', ' ', 'm', 'o', 't', 'i', 'v', 'a', 't', 'i', 'o', 'n', ' ', 'o', 'r', ' ', 'e', 'n', 'e', 'r', 'g', 'y', '.', ' ', 'A', 'n', 'd', ' ', 'd', 'o', 'n', 't', ' ', 'h', 'a', 'v', 'e', ' ', 'a', ' ', 'g', 'i', 'r', 'l', 'f', 'r', 'i', 'e', 'n', 'd', ' ', 's', 't', 'i', 'l', 'l', ' ', 'v', 'i', 'r', 'g', 'i', 'n', ' ', '.', ' ', 'W', 'h', 'y', ' ', 'i', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'u', 'p', ' ', 'f', 'o', 'r', ' ', 'n', 'o', 't', 'h', 'i', 'n', 'g', ' ', '?', ',', ' ', ' ', 'f', 'o', 'r', ' ', 'm', 'o', 'r', 'e', ' ', 's', 'u', 'f', 'f', 'e', 'r', ' ', '?', ' ', 'o', 'r', ' ', 'm', 'o', 'r', 'e', ' ', 'f', 'a', 'i', 'l', 'u', 'r', 'e', ' ', '?', '\\n', 'I', ' ', 'j', 'u', 's', 't', ' ', 'w', 'a', 'n', 't', ' ', 'p', 'e', 'a', 'c', 'e', ' ', ',', ' ', 'l', 'o', 'v', 'e', ' ', 'a', 'n', 'd', ' ', 's', 'o', 'm', 'e', ' ', 'm', 'o', 'n', 'e', 'y', '.', '.', '.', '\\n', 'I', ' ', 'k', 'n', 'o', 'w', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 's', 't', 'i', 'l', 'l', ' ', 's', 'o', 'm', 'e', ' ', 'h', 'o', 'p', 'e', ' ', 'b', 'u', 't', ' ', 'i', ' ', 't', 'i', 'r', 'e', 'd', ' ', 'k', 'e', 'e', 'p', ' ', 'f', 'i', 'g', 'h', 't', 'i', 'n', 'g', ' ', 'i', 't', ' ', 'i', 's', ' ', 'p', 'o', 'i', 'n', 't', 'l', 'e', 's', 's', ' ', ',', ' ', 'i', ' ', 'h', 'a', 't', 'e', ' ', 'i', 't', ' ', 'i', ' ', 'j', 'u', 's', 't', ' ', 'w', 'a', 'n', 't', ' ', 's', 'o', 'm', 'e', ' ', 'v', 'i', 'c', 't', 'o', 'r', 'y', ' ', '.', ' ', 'I', ' ', 'a', 'm', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'g', 'u', 'n', ' ', 'b', 'u', 't', ' ', 'i', 't', ' ', 'i', 's', ' ', 'h', 'a', 'r', 'd', ' ', 't', 'o', ' ', 'a', 'c', 'c', 'e', 's', 's', ' ', 'o', 'n', ' ', 'm', 'y', ' ', 'c', 'o', 'u', 'n', 't', 'r', 'y', ' ', '.', ' ', 'I', ' ', 'j', 'u', 's', 't', ' ', 'd', 'o', 'n', 't', ' ', 'w', 'a', 'n', 't', ' ', 'h', 'u', 'r', 't', ' ', 'a', 'n', 'y', 'm', 'o', 'r', 'e', '.', '.', '.', ' ', 'i', 't', ' ', 'i', 's', ' ', 'e', 'n', 'o', 'u', 'g', 'h', '.', ' ', ' ', 'I', 'f', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'i', 'n', 't', 'e', 'r', 'e', 's', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'm', 'o', 't', 'i', 'v', 'a', 't', 'i', 'o', 'n', 'a', 'l', ' ', 'v', 'i', 'd', 'e', 'o', 's', ' ', 'p', 'l', 'e', 'a', 's', 'e', ' ', 'w', 'a', 't', 'c', 'h', ' ', '\\n', '(', 'W', 'h', 'y', ' ', 'w', 'e', ' ', 'c', 'h', 'o', 'o', 's', 'e', ' ', 's', 'u', 'i', 'c', 'i', 'd', 'e', ' ', 'M', 'a', 'r', 'k', ' ', 'H', 'e', 'n', 'i', 'c', ')', ' ', 'i', 't', ' ', 'r', 'e', 'l', 'a', 'x', 'e', 'd', ' ', 'm', 'e', ' ', 'o', 'n', 'e', ' ', 'b', 'i', 't', ' ', '.', ' ', 'I', ' ', 'n', 'e', 'e', 'd', ' ', 'y', 'o', 'u', 'r', ' ', 'p', 'r', 'a', 'y', 's', ' ', 't', 'o', 'o']\n",
      "I am losing my mind...I do not know how i can endure this bullshit ...  \n",
      "Iam 21 and suffered almost every stage of my life , things are not going on my way , worst thing is everyone hates me even my family too . They think iam a failure.\n",
      "Iam an university student but my grades like an  rotten apple on the tree... i have no motivation or energy. And do not have a girlfriend still virgin . Why i should keep up for nothing ?,  for more suffer ? or more failure ?\n",
      "I just want peace , love and some money...\n",
      "I know there is still some hope but i tired keep fighting it is pointless , i hate it i just want some victory . I am looking for a gun but it is hard to access on my country . I just do not want hurt anymore... it is enough.  If people interested in motivational videos please watch \n",
      "(Why we choose suicide Mark Henic) it relaxed me one bit . I need your prays too\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195700/195700 [14:25<00:00, 225.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(range(len(data_t))):\n",
    "    try:\n",
    "         data_t['text'][t] = contractions.fix(data_t['text'][t])\n",
    "    except:\n",
    "        print(data_t['text'][t])\n",
    "        data_t['text'][t] = contractions.fix(''.join(remove_non_ascii(data_t['text'][t])))\n",
    "        print(data_t['text'][t])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-14a26c13a862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Aplica la eliminación del ruido\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4356\u001b[0m         \"\"\"\n\u001b[0;32m-> 4357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1099\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m                     \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m                 )\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' \\1 \\2 '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# We are not using CONTRACTIONS4 since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_t['text'] = data_t['text'].apply(word_tokenize) #Aplica la eliminación del ruido\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/195700 [00:00<?, ?it/s]/home/valentina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      " 25%|██▌       | 49312/195700 [06:20<20:32, 118.80it/s] "
     ]
    }
   ],
   "source": [
    "for t in tqdm(range(len(data_t))):\n",
    "    try:\n",
    "        data_t['text'][t]=remove_stopwords(remove_non_ascii(remove_punctuation(replace_numbers(to_lowercase(data_t['text'][t])))))\n",
    "    except:\n",
    "        print(t,data_t['text'][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(data_t)):\n",
    "    try:\n",
    "        contractions.fix(data_t['text'][t])\n",
    "    except:\n",
    "        print(data_t['text'][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let us get this bread 😎 Anyone know any good bakery stores? 👀\n"
     ]
    }
   ],
   "source": [
    "print(contractions.fix(data['text'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t['words'] = data_t['Message'].apply(word_tokenize).apply(preprocessing) #Aplica la eliminación del ruido\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         stem_sentence\u001b[38;5;241m.\u001b[39mappend(porter\u001b[38;5;241m.\u001b[39mstem(word))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stem_sentence\n\u001b[0;32m---> 13\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43mstemSentence\u001b[49m(sentence)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_verbs\u001b[39m(words):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stemSentence' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(porter.stem(word))\n",
    "    return stem_sentence\n",
    "\n",
    "x=stemSentence(sentence)\n",
    "print(x)\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    stem_sentence=[]\n",
    "    for word in words:\n",
    "        stem_sentence.append(wordnet_lemmatizer.lemmatize(word))\n",
    "    return stem_sentence\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas\n",
    "\n",
    "data_t['words'] = data_t['words'].apply(stem_and_lemmatize) #Aplica lematización y Eliminación de Prefijos y Sufijos.\n",
    "data_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_t['words'] = data_t['words'].apply(lambda x: ' '.join(map(str, x)))\n",
    "data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data = data_t['words'],data_t['Category']\n",
    "y_data = (y_data == 'spam').astype(int)\n",
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = CountVectorizer(binary=True)\n",
    "X_dummy = dummy.fit_transform(X_data)\n",
    "print(X_dummy.shape)\n",
    "X_dummy.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer()\n",
    "X_count = count.fit_transform(X_data)\n",
    "print(X_count.shape)\n",
    "X_count.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tf_idf = ...\n",
    "\n",
    "print(X_tf_idf.shape)\n",
    "X_tf_idf.toarray()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
